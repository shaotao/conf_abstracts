A Look at Basics of Distributed Computing.,"This tutorial presents concepts and basics of distributed computing which are important (at least from the author's point of view!), and should be known and mastered by Master students, researchers, and engineers. Those include: (a) a characterization of distributed computing (which is too much often confused with parallel computing); (b) the notion of a synchronous system and its associated notions of a local algorithm and message adversaries; (c) the notion of an asynchronous shared memory system and its associated notions of universality and progress conditions; and (d) the notion of an asynchronous messagepassing system with its associated broadcast and agreement abstractions, its impossibility results, and approaches to circumvent them. Hence, the tutorial can be seen as a guided tour to key elements that constitute basics of distributed computing."
Cognitive Technologies for Smarter Cities.,"Great computing technology advances during the early years of the new millennium facilitated a smarter cities vision. Smarter cities solutions focusing on specific city problem domains have proven their value in many proofs-of-concept. The insights and experiences from these experiments enable cities to deploy these types of solutions more widely with confidence. New advances in a broad range of technologies, in particular Internet of Things technologies and cognitive technologies, promise to expand the scope and the value of smarter cities solutions to greatly improve the lives of city dwellers. This tutorial gives an overview of smarter cities applications so far, and the technologies powering them. Then it proceeds to review the incipient technologies that will drive the next wave of smarter cities solutions: the Internet of Things driving cognitive computing systems. It concludes by examining two major smarter cities solution categories that promise to greatly improve the lives of city dwellers."
"Reflecting on the Past, Preparing for the Future: From Peer-to-Peer to Edge-Centric Computing.","In many aspects of human activity, there has been a continuous struggle between the forces of centralization and decentralization. Computing exhibits the same phenomenon; after having abandoned mainframes in favor of PCs, the last decade has witnessed an unparalleled centralization and consolidation of services in data centers and clouds. Yet, trust, privacy, security and autonomy concerns are requiring to shift control again, taking services from the central nodes (the ""core"") to the other logical extreme (the ""edge"") of the Internet. This development can help blurring the boundary between man and machine, and embrace social computing in which humans are part of the computation and decision-making loop, resulting in a human-centered system design. In this tutorial we will elaborate on the necessary steps to be taken and challenges to be solved to realize this vision. The tutorial will include an overview of related research topics, including peerto-peer networks, blockchains, hybrid and decentralized cloud architectures."
Software-Based Networks: Leveraging High-Performance NFV Platforms to Meet Future Communication Challenges.,"Summary form only given. Communication networks are changing: they are becoming more and more ""software-based."" The use of network function virtualization (NFV) to run network services in software enables software-defined networks (SDNs) to create a largely software-based network. To truly achieve the vision of a high-performance software-based network that is flexible, lower- cost, and agile, a fast and carefully designed network function virtualization platform along with a comprehensive SDN control plane is needed.Our high-performance NFV platform, OpenNetVM, enables high bandwidth network functions to operate at near line speed, while taking advantage of the flexibility and customization of low cost commodity servers. OpenNetVM exploits Intel's DPDK libraries to minimize the overhead of packet processing, and to provide high throughput, low latency networking in virtualized environments. OpenNetVM allows true zero-copy delivery of data to VMs, both for packet processing and high-speed inter-VM communication through shared huge pages within a trust boundary. We envision a dynamic and flexible network that can support a smarter data plane than just simple switches that forward packets. We build on our OpenNetVM NFV platform by developing our SDNFV network architecture that supports complex stateful routing of flows where processing by network functions (NFs) can dynamically modify the path taken by flows, without unduly burdening the centralized SDN controller. The tutorial will also briefly touch upon the problem of dynamic placement of network functions and routing of flows through a software based network, exploiting a mixture of centralized SDN control and NFV capabilities in the network. The problem can be formulated as a mixed integer linear programming problem, and heuristics can be used to solve the problem incrementally.As a case study, we examine the growing communication needs of 'Internet-of-Things' (IoT). With 'smart' sensing devices becoming ubiquitous, there is a need to support IoT communication at large scale, especially over cellular networks. The use of NFV platforms for the Evolved Packet Core, we see opportunities for supporting IoT communications at large scale in 5G cellular networks. We describe potential solutions in this direction."
INSPECTOR: Data Provenance Using Intel Processor Trace (PT).,"Data provenance strives for explaining how the computation was performed by recording a trace of the execution. The provenance trace is useful across a wide-range of workflows to improve the dependability, security, and efficiency of software systems. In this paper, we present Inspector, a POSIX-compliant data provenance library for shared-memory multithreaded programs. The Inspector library is completely transparent and easy to use: it can be used as a replacement for the pthreads library by a simple exchange of libraries linked, without even recompiling the application code. To achieve this result, we present a parallel provenance algorithm that records control, data, and schedule dependencies using a Concurrent Provenance Graph (CPG). We implemented our algorithm to operate at the compiled binary code level by leveraging a combination of OS-specific mechanisms, and recently released Intel PT ISA extensions as part of the Broadwell micro-architecture. Our evaluation on a multicore platform using applications from multithreaded benchmarks suites (PARSEC and Phoenix) shows reasonable provenance overheads for a majority of applications. Lastly, we briefly describe three case-studies where the generic interface exported by Inspector is being used to improve the dependability, security, and efficiency of systems. The Inspector library is publicly available for further use in a wide range of other provenance workflows."
PAG: Private and Accountable Gossip.,"A large variety of content sharing applications rely, at least partially, on gossip-based dissemination protocols. However, these protocols are subject to various types of faults, among which selfish behaviours performed by nodes that benefit from the system without contributing their fair share to it. Accountability mechanisms (e.g., PeerReview, AVMs, FullReview), which require that nodes log their interactions with others and periodically inspect each others' logs are effective solutions to deter faults. However, these solutions require that nodes disclose the content of their logs, which may leak sensitive information about them. Building on a monitoring infrastructure and on homomorphic cryptographic procedures, we propose in this paper PAG, the first accountable and partially privacy-preserving gossip protocol. We assess PAG theoretically using the ProVerif cryptographic protocol verifier and evaluate it experimentally using both a real deployment on a cluster of 48 machines and simulations. The performance evaluation of PAG, performed using a video live streaming application, shows that it is compatible with the visualisation of live video content on commodity Internet connections. Furthermore, PAG's bandwidth consumption inherits the desirable scalability properties of gossip when the number of users in the system grows."
Practical Intrusion-Tolerant Networks.,"As the Internet becomes an important part of the infrastructure our society depends on, it is crucial to construct networks that are able to work even when part of the network is compromised. This paper presents the first practical intrusion-tolerant network service, targeting high-value applications such as monitoring and control of global clouds and management of critical infrastructure for the power grid. We use an overlay approach to leverage the existing IP infrastructure while providing the required resiliency and timeliness. Our solution overcomes malicious attacks and compromises in both the underlying network infrastructure and in the overlay itself. We deploy and evaluate the intrusion-tolerant overlay implementation on a global cloud spanning East Asia, North America, and Europe, and make it publicly available."
Gremlin: Systematic Resilience Testing of Microservices.,"Modern Internet applications are being disaggregated into a microservice-based architecture, with services being updated and deployed hundreds of times a day. The accelerated software life cycle and heterogeneity of language runtimes in a single application necessitates a new approach for testing the resiliency of these applications in production infrastructures. We present Gremlin, a framework for systematically testing the failure-handling capabilities of microservices. Gremlin is based on the observation that microservices are loosely coupled and thus rely on standard message-exchange patterns over the network. Gremlin allows the operator to easily design tests and executes them by manipulating inter-service messages at the network layer. We show how to use Gremlin to express common failure scenarios and how developers of an enterprise application were able to discover previously unknown bugs in their failure-handling code without modifying the application."
CRONets: Cloud-Routed Overlay Networks.,"Overlay networking and ISP-assisted tunneling are effective solutions to overcome problematic BGP routes and bypass troublesome autonomous systems. Despite their demonstrated effectiveness, overlay support is not broadly available. In this paper, we propose Cloud-Routed Overlay Networks (CRONets), whereby users can readily build their own overlays using nodes from global and well-provisioned cloud providers like IBM Softlayer or Amazon EC2. While previous studies have demonstrated the benefits of overlay networks with the high-speed experimental Internet2 backbone, we are the first to evaluate the improvements in a realistic -- cloud -- setting. We conduct a large-scale experiment where we observe 6,600 Internet paths. The results show that CRONets improve the throughput for 78% of the default Internet paths with a median and average improvement factors of 1.67 and 3.27 times respectively, at a tenth of the cost of leasing private lines of comparable performance. We also performed a longitudinal measurement, and demonstrate that the performance gains are consistent over time with only a small number of overlay nodes needed to be deployed. However, given the size and dynamic nature of the Internet routing system (e.g., due to congestion and failures), selecting the proper path is still a challenging problem. To address it, we propose a novel solution based on the newly-introduced MPTCP extensions. Our experiments show that MPTCP can achieve the maximum observed throughput across the different overlay paths."
Riptide: Jump-Starting Back-Office Connections in Cloud Systems.,"Large-scale cloud networks are constantly driven by the need for improved performance in communication between datacenters. Indeed, such back-office communication makes up a large fraction of traffic in many cloud environments. This communication often occurs frequently, carrying control messages, coordination and load balancing information, and customer data. However, ensuring such inter-datacenter traffic is delivered efficiently requires optimizing connections over large physical distances, which is non-trivial. Worse still, many large cloud networks are subject to complex configuration and administrative restrictions, limiting the types of solutions that can be implemented. In this paper, we propose improving the efficiency of datacenter to datacenter communication by learning the congestion level of links in between. We then use this knowledge to inform new connections made between the relevant datacenters, allowing us to eliminate the overhead associated with traditional slow-start processes in new connections. We further present Riptide, a tool which implements this approach. We present the design and implementation details of Riptide, showing that it can be easily executed on modern Linux servers deployed in the real world. We further demonstrate that it successfully reduces total transfer times in a production global-scale content delivery network (CDN), providing up to a 30% decrease in tail latency. We further show that Riptide is simple to deploy and easy to maintain within a complex existing network."
The Internet is for Porn: Measurement and Analysis of Online Adult Traffic.,"Adult (or pornographic) websites attract a large number of visitors and account for a substantial fraction of the global Internet traffic. However, little is known about the makeup and characteristics of online adult traffic. In this paper, we present the first large-scale measurement study of online adult traffic using HTTP logs collected from a major commercial content delivery network. Our data set contains approximately 323 terabytes worth of traffic from 80 million users, and includes traffic from several dozen major adult websites and their users in four different continents. We analyze several characteristics of online adult traffic including content and traffic composition, device type composition, temporal dynamics, content popularity, content injection, and user engagement. Our analysis reveals several unique characteristics of online adult traffic. We also analyze implications of our findings on adult content delivery. Our findings suggest several content delivery and cache performance optimizations for adult traffic, e.g., modifications to website design, content delivery, cache placement strategies, and cache storage configurations."
Tuning the Aggressive TCP Behavior for Highly Concurrent HTTP Connections in Data Center.,"Modern data centers host diverse HTTP-based services, which employ persistent TCP connections to send HTTP requests and responses. However, the ON/OFF pattern of HTTP traffic disturbs the increase of TCP congestion window, potentially triggering packet loss at the beginning of ON period. Furthermore, the transmission performance becomes worse due to severe congestion in the concurrent transfer of HTTP response. In this work, we first reveal that the TCP's aggressive behavior in increasing congestion window causes TCP timeouts and throughput collapse. We further present the design and implementation of TCP-TRIM, which employs probe packets to smooth the aggressive increase of congestion window in persistent TCP connection, and leverages congestion detection and control at end-host to limit the growth of switch queue length under highly concurrent TCP connections. The experimental results of at-scale simulations and real implementations show that TCPTRIM reduces the completion time of HTTP response by up to 80%, while introducing little deployment overhead only at the end hosts."
A Performance Analysis of Incentive Mechanisms for Cooperative Computing.,"As more devices gain Internet connectivity, more information needs to be exchanged between them. For instance, cloud servers might disseminate instructions to clients, or sensors in the Internet of Things might send measurements to each other. In such scenarios, information spreads faster when users have an incentive to contribute data to others. While many works have considered this problem in peer-to-peer scenarios, none have rigorously theorized the performance of different design choices for the incentive mechanisms. In particular, different designs have different ways of ""bootstrapping"" new users (distributing information to them) and preventing ""free-riding"" (receiving information without uploading any in return). We classify incentive mechanisms in terms of reciprocity-, altruism-, and reputation-based algorithms, and then analyze the performance of these three basic and three hybrid algorithms. We show that the algorithms lie along a tradeoff between fairness and efficiency, with altruism and reciprocity at the two extremes. The three hybrids all leverage their component algorithms to achieve similar efficiency. The reputation hybrids are the most fair and can nearly match altruism's bootstrapping speed, but only the reciprocity/reputation hybrid can match reciprocity's zero-tolerance for free-riding. It therefore yields better fairness and efficiency when free-riders are present. We validate these comparisons with extensive experimental results."
GrapH: Heterogeneity-Aware Graph Computation with Adaptive Partitioning.,"Vertex-centric graph processing systems such as Pregel, PowerGraph, or GraphX recently gained popularity due to their superior performance of data analytics on graph-structured data. These systems exploit the graph structure to improve data access locality during computation, making use of specialized graph partitioning algorithms. Recent partitioning techniques assume a uniform and constant amount of data exchanged between graph vertices (i.e., uniform vertex traffic) and homogeneous underlying network costs. However, in real-world scenarios vertex traffic and network costs are heterogeneous. This leads to suboptimal partitioning decisions and inefficient graph processing. To this end, we designed GrapH, the first graph processing system using vertex-cut graph partitioning that considers both, diverse vertex traffic and heterogeneous network, to minimize overall communication costs. The main idea is to avoid frequent communication over expensive network links using an adaptive edge migration strategy. Our evaluations show an improvement of 60% in communication costs compared to state-of-the-art partitioning approaches."
Minimum-Cost Cloud Storage Service Across Multiple Cloud Providers.,"Many Cloud Service Providers (CSPs) provide data storage services with datacenters distributed worldwide. These datacenters provide different Get/Put latencies and unit prices for resource utilization and reservation. Thus, when selecting different CSPs' datacenters, cloud customers of globally distributed applications (e.g., online social networks) face two challenges: (i) how to allocate data to worldwide datacenters to satisfy application SLO (service level objective) requirements including both data retrieval latency and availability, and (ii) how to allocate data and reserve resources in datacenters belonging to different CSPs to minimize the payment cost. To handle these challenges, we first model the cost minimization problem under SLO constraints using integer programming. Due to its NP-hardness, we then introduce our heuristic solution, including a dominant-cost based data allocation algorithm and an optimal resource reservation algorithm. We finally introduce an infrastructure to enable the conduction of the algorithms. Our trace-driven experiments on a supercomputing cluster and on real clouds (i.e., Amazon S3, Windows Azure Storage and Google Cloud Storage) show the effectiveness of our algorithms for SLO guaranteed services and customer cost minimization."
A Realistic and Optimized V2V Communication System for Taxicabs.,"Due to high mobility and intermittent connections in vehicular networks, reliable and efficient vehicular communication is a challenging task. Previous research on Vehicle-to-Vehicle (V2V) communication mostly focuses on achieving reliable transmissions from a given source to a given destination by mining moving patterns of taxicabs. However, to the best of our knowledge, none of them considered the habit-driven regularities of individual taxicabs as well as the urban-layout-driven time-varying regularities of crowds of taxicabs synthetically. With this insight, we model both individual and holistic driving patterns by Markov Chain models, then devise a new method to predict possible driving routes for every single taxicab. In addition, we design a new method to evaluate the probability that a single taxicab retrieves information of a specific road segment while it drives through another road segment during a given time period, and also to quantify the expected probability that a single taxicab obtains the information of a given road segment in the near future. With such information, our solution enables the selection of the optimal data packet transmission scheme. We evaluate our solution on a real-world taxicab dataset. Experimental results demonstrate that our approach outperforms alternative solutions in terms of diffusion speed and success ratio of data retrieval."
Optimal Marching of Autonomous Networked Robots.,"The recent advances in sensors, actuators, robots, and mobile wireless communication technologies have acceleratedinterest in autonomous networked robots (ANRs), where theindividual robots coordinate among themselves to complete atask, e.g., to explore or monitor a Field of Interest (FoI). Byteamwork, which is especially important in complex tasks, ANRsystem expresses much more capacity than traditional staticsensor networks. Existing work focuses on improving the coverageperformance of a group of ANRs within a single FoI. In thisresearch, we consider a group of ANRs that are instructed toexplore a number of FoIs. After they complete a task at currentFoI, they move to the next one, which may be far away fromcurrent one and the shape can also vary dramatically. Ourresearch focuses on how to efficiently enable such transition. TheANRs must be able to redeploy themselves to desired positionsin the new FoI based on distributed algorithms. Besides, to avoidunexpected event breaks network's integrity, the ANRs shouldpreserve their local connectivities as much as they can andorganize themselves as a whole network without any isolatednodes during the transition. Furthermore, considering energyconsumption, such relocation algorithm should work at the costof reasonable total moving distance. We study this problemand call it optimal marching of autonomous networked robots. The proposed algorithms guarantee global connectivity, andpreserve local connectivities as much as possible at negligiblecost of moving distance. Additionally, ANRs can automaticallyadjust their deployment density in the new FoI based on therequirements of various tasks or regions."
RichNote: Adaptive Selection and Delivery of Rich Media Notifications to Mobile Users.,"In recent years, notification services for social networks, mobile apps, messaging systems and other electronic services have become truly ubiquitous. When a new content becomes available, the service sends an instant notification to the user. When the content is produced in massive quantities, and it includes both large-size media and a lot of meta-information, it gives rise to a major challenge of selecting content to notify about and information to include in such notifications. We tackle three important challenges in realizing rich notification delivery: (1) content and presentation utility modeling, (2) notification selection and (3) scheduling of delivery. We consider a number of progressive presentation levels for the content. Since utility is subjective and hard to model, we rely on real data and user surveys. We model the content utility by learning from large-scale real world data collected from Spotify music streaming service. For the utility of the presentation levels we rely on user surveys. Blending these two techniques together, we derive utility of notifications with different presentation levels. We then model the selection and delivery of rich notifications as an optimization problem with a goal to maximize the utility of notifications under resource budget constraints. We validate our system with large-scale simulations driven by the real-world de-identified traces obtained from Spotify. With the help of several baseline approaches we show that our solution is adaptive and resource efficient."
Decentralized Context Sharing in Vehicular Delay Tolerant Networks with Compressive Sensing.,"Vehicles equipped with various types of sensors can act as mobile sensors to monitor the road conditions. To speed up the information collection process, the monitoring data can be shared among vehicles upon their encounters to facilitate drivers to find a good route. The vehicular network experiences intermittent connectivity as a result of the mobility, which makes the inter-vehicle contact duration a scarce resource for data transmissions and the support of monitoring applications over vehicular networks a challenge. We propose a novel compressive sensing (CS)-based scheme to enable efficient decentralized context sharing in vehicular delay tolerant networks, called CS-Sharing. To greatly reduce the data transmission overhead and speed up the monitoring processing, CS-sharing exploits two techniques: sending an aggregate message in each vehicle encounter, and quick collection of information taking advantage of data sharing and the sparsity of events in vehicle networks to significantly reduce the number of measurements needed for global information recovery. We propose a novel data structure, and an aggregation method that can take advantage of the random and opportunistic vehicle encounters to form the measurement matrix. We prove that the measurement matrix satisfies the Restricted Isometry Property (RIP) property required by the CS technique. Our results from extensive simulations demonstrate that CS-Sharing allows vehicles in a large network to quickly obtain the full context data with the successful recovery ratio larger than 90%."
RuleTris: Minimizing Rule Update Latency for TCAM-Based SDN Switches.,"Software-dehned network (SDN) is deemed to enable more dynamic management of data center networks that promptly respond to network events with changes in network policies. Although the SDN controller architecture is increasingly optimized for swift policy updates, the data plane, especially the prevailing TCAM-based flow tables on physical SDN switches, remains unoptimized for fast rule updates, and is gradually becoming the primary bottleneck along the policy update pipeline. In this paper, we present RuleTris, the hrst SDN update optimization framework that minimizes rule update latency for TCAM-based switches. RuleTris employs the dependency graph (DAG) as the key abstraction to minimize the update latency. RuleTris efhciently obtains the DAGs with novel dependency preserving algorithms that incrementally build rule dependency along with the compilation process. Then, in the guidance of the DAG, RuleTris optimizes the rule updates in TCAM to avoid unnecessary entry moves, which are the main cause of TCAM update inefhciency. We prove that RuleTris generates TCAM updates with the minimum number of TCAM entry moves. In evaluation, RuleTris achieves a median of <;12ms and 90-percentile of <;15ms the end-to-end per-rule update latency on our hardware prototype, outperforming the state-of-the-art composition compiler CoVisor by ~20 times."
RITM: Revocation in the Middle.,"Although TLS is used on a daily basis by many critical applications, the public-key infrastructure that it relies on still lacks an adequate revocation mechanism. An ideal revocation mechanism should be inexpensive, efficient, secure, and privacypreserving. Moreover, rising trends in pervasive encryption pose new scalability challenges that a modern revocation system should address. In this paper, we investigate how network nodes can deliver certificate-validity information to clients. We present RITM, a framework in which middleboxes (as opposed to clients, servers, or certification authorities) store revocation-related data. RITM provides a secure revocation-checking mechanism that preserves user privacy. We also propose to take advantage of content-delivery networks (CDNs) and argue that they would constitute a fast and cost-effective way to disseminate revocations. Additionally, RITM keeps certification authorities accountable for the revocations that they have issued, and it minimizes overhead at clients and servers, as they have to neither store nor download any messages. We also describe feasible deployment models and present an evaluation of RITM to demonstrate its feasibility and benefits in a real-world deployment."
A Distributed Auctioneer for Resource Allocation in Decentralized Systems.,"In decentralized systems, nodes often need to coordinate to access shared resources in a fair manner. One approach to perform such arbitration is to rely on auction mechanisms. Although there is an extensive literature that studies auctions, most of these works assume the existence of a central, trusted auctioneer. Unfortunately, in fully decentralized systems, where the nodes that need to cooperate operate under separate spheres of control, such central trusted entity may not exist. Notable examples of such decentralized systems include community networks, clouds of clouds, cooperative nano data centres, among others. In this paper, we make theoretical and practical contributions to distribute the role of the auctioneer. From the theoretical perspective, we propose a framework of distributed simulations of the auctioneer that are Nash equilibria resilient to coalitions and asynchrony. From the practical perspective, our protocols leverage the distributed nature of the simulations to parallelise the execution. We have implemented a prototype that instantiates the framework for bandwidth allocation in community networks, and evaluated it in a real distributed setting."
Datacomp: Locally Independent Adaptive Compression for Real-World Systems.,"Non-lossy compression can save time and energy during communication if the cost to compress and send input is less than the cost of sending it uncompressed. Unfortunately, compression can also degrade performance, no single method is always beneficial, and outcomes depend on many factors. As a result, compression choices in real systems are coarsely grained and manually controlled, resulting in suboptimal or even poor performance. Adaptive Compression (AC) systems make compression choices dynamically to optimize utility. Existing AC systems are limited in ways that reduce their suitability for general-purpose computers. Datacomp is an AC system that operates locally and includes no significant hard-coded knowledge. Using real-world data, a broad range of environments and the Comptool ""AC Oracle,"" we show that Datacomp's performance is equivalent or close to the ideal at bandwidths between 1-100Mbit/s, even when static strategies are suboptimal or more costly than no compression. While Datacomp struggles to perform well at 1Gbit/s, understanding why illustrates important challenges for AC systems and suggests solutions."
Hybrid Content-Based Routing Using Network and Application Layer Filtering.,"Over the past few decades, content-based publish/subscribe has been primarily implemented as an overlay network of software brokers. Even though such systems provide the possibility of bandwidth efficient expressive filtering in software, they cannot match up to the performance (in terms of end-to-end latency and throughput) of communication protocols implemented on the network layer. To exploit network layer performance benefits, recently, content-based publish/subscribe was realized using the capabilities of Software-defined Networking (SDN). While SDN allows line-rate forwarding of events by content filters directly installed on switches, it suffers from inherent hardware limitations (w.r.t. flow table size, limited availability of bits in header fields) that adversely impact expressiveness of these filters, resulting in unnecessary network traffic. In this paper, we strike a balance between purely application-layer-based and purely network-layer-based publish/subscribe implementations by realizing the first hybrid content-based middleware that enables filtering of events in both layers. Moreover, we provide different selection algorithms with varying degrees of complexity to determine the events to be filtered at each layer such that unnecessary network traffic can be minimized while also considering delay requirements of the middleware. Our hybrid middleware offers full flexibility to configure it according to the performance requirements of the system. We provide a detailed performance evaluation of the proposed selection algorithms to determine their impact on the performance of the designed hybrid middleware which we further compare to state-of-the art solutions."
Exploiting Causality to Engineer Elastic Distributed Software.,"This goal of this paper is to anchor elasticity in terms of causality in distributed applications. Assuming a large-scale distributed application architected as a set of interacting components, we motivate the need for (1) analyzing causality between interactions, and (2) estimating casual probability that an increase in the frequency of interaction i
<sub xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">1</sub>
 can increase the frequency of interaction i
<sub xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">2</sub>
 caused by i
<sub xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">1</sub>
. We present algorithms to estimate causality and causal probability by combining well known sampling, program analysis, path profiling and dynamic slicing algorithms. We apply our algorithms for causal probability to three large-scale distributed applications, to evaluate (a) their effectiveness in the timely provisioning and de-provisioning of compute resources and (b) whether causality and causal probability present a fundamental and widely-applicable way of engineering auto-elasticity."
RUSH: A RobUst ScHeduler to Manage Uncertain Completion-Times in Shared Clouds.,"We address the problem of scheduling jobs with utilities that depend solely upon their completion-times in a shared cloud that imposes considerable uncertainty on the jobs' runtime. However, it is very hard to estimate the jobs' runtime in a shared cloud where jobs are often delayed due to reasons such as slow I/O performance and variations in memory availability. Unlike prior works, we acknowledge that runtime estimates are often erroneous and instead shift the burden of robustness to the job scheduler. Specifically, we present a scheduling problem that jointly accounts for: (i) job utilities specified as functions of their completion-time, and (ii) uncertainty in the jobs' runtime. Our proposed solution to this problem achieves lexicographic max-min fairness among the job utilities. We implement this as a robust scheduler, named RUSH, for YARN in Hadoop. Our experiments, using real-world data sets, illustrate RUSH's efficacy when compared with other commonly used schedulers."
The Same Speed Timer in Population Protocols.,"A novel concept of the same speed timer is presented, and is applied in the population protocol (PP) model to improve the convergence time of existing loosely-stabilizing leader election protocols. Loosely-stabilizing leader election guarantees that, starting from any configuration, the system reaches a safe configuration within a short time (convergence), and after that, the system keeps the unique leader for a long time (closure). Two loosely-stabilizing leader election protocols for arbitrary graphs exist in the literature; one uses identifiers of nodes and the other uses random numbers to elect a unique leader. Both protocols guarantee that the expected convergence time is polynomial and the expected holding time (the time the leader is kept) is exponential. In this paper, convergence time of these protocols is dramatically improved by the same speed timer without impairing the exponential holding time. Specifically, a fast deterministic loosely-stabilizing leader election protocol that uses identifiers of nodes and a fast randomized looselystabilizing leader election protocol are given. The expected convergence time and expected holding time of the former protocol are O(mN log N) and 立(Ne
<sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">2N</sup>
), respectively, where m is the number of edges in the graph and N is a given upper bound on the number of nodes n. The expected convergence time and expected holding time of the latter protocol are O(mN
<sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">2</sup>
 log n) and 立(Ne
<sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">2N</sup>
), respectively. A self-stabilizing two-hop coloring protocol that uses only O(log n) memory space of each agent is given as a tool of the latter protocol. A lower bound is also given: any loosely-stabilizing leader election protocol with expected exponential holding time requires 立(mN) expected convergence time."
Interactive Wireless Charging for Energy Balance.,"Wireless energy transfer is an emerging technology that is used in networks of battery-powered devices in order to deliver energy and keep the network functional. Existing state-of-the-art studies have mainly focused on applying this technology on networks of relatively strong computational and communicational capabilities (wireless sensor networks, ad-hoc networks), also they assume one-directional energy transfer from special chargers to the network nodes. Different from these works, we here study (for the first time in the state-of-theart) interactive, ""peer-to-peer"" wireless charging in populations of much more resource-limited, mobile agents that abstract distributed portable devices. In this new model for interactive wireless charging, we assume that the agents are capable of achieving bi-directional wireless energy transfer acting both as energy transmitters and harvesters. We consider the cases of both loss-less and lossy energy transfer and provide an upper bound on the time needed to reach a balanced energy distribution in the population. We investigate the delicate impact of the diversity of energy levels on eventual energy balance achieved and highlight some key elements of the charging procedure. In the light of the above, we design and evaluate three interaction protocols that achieve different tradeoffs between energy balance, time and energy efficiency."
Nearly Optimal Distributed Algorithm for Computing Betweenness Centrality.,"In this paper, we propose an O(N) time distributed algorithm for computing betweenness centralities of all nodes in the network where N is the number of nodes. Our distributed algorithm is designed under the widely employed CONGEST model in the distributed computing community which limits each message only contains O(log N) bits. To our best knowledge, this is the first linear time deterministic distributed algorithm for computing the betweenness centralities in the published literature. We also give a lower bound for distributively computing the betweenness centrality under the CONGEST model as 立(D+N/ log N) where D is the diameter of the network. This implies that our distributed algorithm is nearly optimal."
An Efficient Lock-Free Logarithmic Search Data Structure Based on Multi-dimensional List.,"Logarithmic search data structures, such as search trees and skiplists, are fundamental building blocks of many applications. Although the self-balancing binary search trees are among the most ubiquitous sequential search data structures, designing non-blocking rebalancing algorithms is challenging due to the required structural alternation, which may stall other concurrent operations. Skiplists, which probabilistically create multiple levels of shortcuts in an ordered list, provide practical alternatives to balanced search trees. The use of skiplists eliminates the need of rebalancing and ensures amortized logarithmic sequential search time, but concurrency is limited under write-dominated workload because the linkage between multiple distant nodes must be updated. In this paper, we present a linearizable lock-free dictionary design based on a multi-dimensional list (MDList). A node in an MDList arranges its child nodes by their dimensionality and order them by coordinate prefixes. The search operation works by first generating a one-to-one mapping from the scalar keys to a high-dimensional vectors space, then uniquely locating the target position by using the vector as coordinates. Our algorithm guarantees worst-case search time of O(log N) where N is the size of key space. Moreover, the ordering property of the data structure is readily maintained during mutations without rebalancing nor randomization. In our experimental evaluation using a micro-benchmark, our dictionary outperforms the state of the art approaches by as much as 100% when the key universe is large and an average of 30% across all scenarios."
Characterizing the Computational Power of Anonymous Mobile Robots.,"The distributed setting of computational mobile entities, called robots, thathave to perform tasks without global coordination has been extensively studied in the literature. A well-known scenario is that in which robots operate in Look-Compute-Move (LCM) cycles. During each cycle, a robot acquires asnapshot of the surrounding environment (Look phase), then executes an appropriate algorithm by using the obtained snapshot as input (Computephase), and finally moves toward a desired destination, if any (Movephase). Look-Compute-Move cycles might be subject to different temporal constraints dictated by the considered schedule. The classic models for theactivation and synchronization of mobile robots are the well-known fully-synchronous, semi-synchronous, and asynchronous models. A first comprehensive evaluation of the computational power of robots operating in the LCM model and moving within the Euclidean plane, under different levels of synchronization, has been proposed in [Das et al., Int.'l Conf. on Distributed Computing Systems, 2012]. In detail, the authors provide a series of results that prove relations between classic models and variations of them, which consider the possibility that robots are endowed with a visible light, i.e. they are luminous, or with the capability to store some past snapshots, or combinations of them. In this paper, we are interested in similar settings but for robots moving on graphs. In particular, we propose a characterization of the computational power of mobile robots on graphs as follows. First, we show the relations among the three classic activation and synchronization models. Second, we compare the models where robots are endowed with lights against the models without lights. Third, we highlight the relations among the different models concerning luminous robots. Finally, we provide a detailed comparison of the proposed results with the case of robots moving in the Euclidean plane."
CryptoLock (and Drop It): Stopping Ransomware Attacks on User Data.,"Ransomware is a growing threat that encrypts auser's files and holds the decryption key until a ransom ispaid by the victim. This type of malware is responsible fortens of millions of dollars in extortion annually. Worse still, developing new variants is trivial, facilitating the evasion of manyantivirus and intrusion detection systems. In this work, we presentCryptoDrop, an early-warning detection system that alerts a userduring suspicious file activity. Using a set of behavior indicators, CryptoDrop can halt a process that appears to be tampering witha large amount of the user's data. Furthermore, by combininga set of indicators common to ransomware, the system can beparameterized for rapid detection with low false positives. Ourexperimental analysis of CryptoDrop stops ransomware fromexecuting with a median loss of only 10 files (out of nearly5,100 available files). Our results show that careful analysis ofransomware behavior can produce an effective detection systemthat significantly mitigates the amount of victim data loss."
Amnesia: A Bilateral Generative Password Manager.,"While numerous flaws have been recognized in using passwords as a method of authentication, passwords still remain the de-facto authentication standard in use today. Though password managers can ameliorate password fatigue, the vast majority of password managers require the user to choose and maintain a strong master password while offering little to no recourse in the event that the master password is compromised. The wide-application of cloud-based password managers congregate passwords in an encrypted database, which becomes an attractive target for attackers and also represents a single point of failure. In this paper, we propose Amnesia, a bilateral generative password manager that requires both the knowledge of the master password and the possession of the user's smartphone to generate website passwords for the user. Our generative password manager is not vulnerable to the password database leakage, since it generates the requested password on demand using both the master password and the secret information on the smartphone. An attacker wishing to steal the user's website passwords has to compromise both the user's smartphone and the master password. Amnesia also has strong recovery capability when either the master password is compromised or the smartphone is lost/stolen. By using an Amnesia server, a user can have the access to the password manager on multiple computers without installing any software on those computers. We implemented an Amnesia system prototype using Android and Cherrypy web framework and evaluated it in terms of security, usability, and overhead. A user study of 31 testers shows that Amnesia increases password security while maintaining reasonable user convenience."
Know Your Phish: Novel Techniques for Detecting Phishing Sites and Their Targets.,"Phishing is a major problem on the Web. Despite the significant attention it has received over the years, there has been no definitive solution. While the state-of-the-art solutions have reasonably good performance, they require a large amount of training data and are not adept at detecting phishing attacks against new targets. In this paper, we begin with two core observations: (a) although phishers try to make a phishing webpage look similar to its target, they do not have unlimited freedom in structuring the phishing webpage, and (b) a webpage can be characterized by a small set of key terms, how these key terms are used in different parts of a webpage is different in the case of legitimate and phishing webpages. Based on these observations, we develop a phishing detection system with several notable properties: it requires very little training data, scales well to much larger test data, is language-independent, fast, resilient to adaptive attacks and implemented entirely on client-side. In addition, we developed a target identification component that can identify the target website that a phishing webpage is attempting to mimic. The target detection component is faster than previously reported systems and can help minimize false positives in our phishing detection system."
BotMeter: Charting DGA-Botnet Landscapes in Large Networks.,"Recent years have witnessed a rampant use of domain generation algorithms (DGAs) in major botnet crimewares, which tremendously strengthens a botnet's capability to evade detection or takedown. Despite a plethora of existing studies on detecting DGA-generated domains in DNS traffic, remediating such threats still relies on vetting the DNS behavior of each individual device. Yet, in large networks featuring complicated DNS infrastructures, we often lack the capability or the resource to exhaustively investigate every part of the networks to identify infected devices in a timely manner. It is therefore of great interest to first assess the population distribution of DGA-bots inside the networks and to prioritize the remediation efforts. In this paper, we present BotMeter, a novel tool that accurately charts the DGA-bot population landscapes in large networks. Specifically, we embrace the prevalent yet challenging setting of hierarchical DNS infrastructures with caching and forwarding mechanisms enabled, whereas DNS traffic is observable only at certain upper-level vantage points. We establish a new taxonomy of DGAs that captures their characteristic DNS dynamics. This allows us to develop a rich library of rigorous analytical models to describe the complex relationships between bot populations and DNS lookups observed at vantage points. We provide results from extensive empirical studies using both synthetic data and real DNS traces to validate the efficacy of BotMeter."
Enabling Privacy-Preserving Incentives for Mobile Crowd Sensing Systems.,"Recent years have witnessed the proliferation of mobile crowd sensing (MCS) systems that leverage the public crowd equipped with various mobile devices (e.g., smartphones, smartglasses, smartwatches) for large scale sensing tasks. Because of the importance of incentivizing worker participation in such MCS systems, several auction-based incentive mechanisms have been proposed in past literature. However, these mechanisms fail to consider the preservation of workers' bid privacy. Therefore, different from prior work, we propose a differentially private incentive mechanism that preserves the privacy of each worker's bid against the other honest-but-curious workers. The motivation of this design comes from the concern that a worker's bid usually contains her private information that should not be disclosed. We design our incentive mechanism based on the single-minded reverse combinatorial auction. Specifically, we design a differentially private, approximately truthful, individual rational, and computationally efficient mechanism that approximately minimizes the platform's total payment with a guaranteed approximation ratio. The advantageous properties of the proposed mechanism are justified through not only rigorous theoretical analysis but also extensive simulations."
Quality-Aware and Fine-Grained Incentive Mechanisms for Mobile Crowdsensing.,"Limited research efforts have been made for Mobile CrowdSensing (MCS) to address quality of the recruited crowd, i.e., quality of services/data each individual mobile user and the whole crowd are potentially capable of providing, which is the main focus of the paper. Moreover, to improve flexibility and effectiveness, we consider fine-grained MCS, in which each sensing task is divided into multiple subtasks and a mobile user may make contributions to multiple subtasks. In this paper, we first introduce mathematical models for characterizing the quality of a recruited crowd for different sensing applications. Based on these models, we present a novel auction formulation for quality-aware and fine-grained MCS, which minimizes the expected expenditure subject to the quality requirement of each subtask. Then we discuss how to achieve the optimal expected expenditure, and present a practical incentive mechanism to solve the auction problem, which is shown to have the desirable properties of truthfulness, individual rationality and computational efficiency. We conducted trace-driven simulation using the mobility dataset of San Francisco taxies. Extensive simulation results show the proposed incentive mechanism achieves noticeable expenditure savings compared to two well-designed baseline methods, and moreover, it produces close-to-optimal solutions."
Toward Optimal DoS-Resistant Authentication in Crowdsensing Networks via Evolutionary Game.,"With the increasing demand of Quality of Service(QoS) in Crowdsensing Networks, providing broadcast authentication and preventing Denial of Service (DoS) attacks become not only a fundamental issue but also a challenging security service. The multi-level TESLA is a series of lightweight broadcast authentication protocols, which can effectively mitigate DoS attacks via randomly selected messages. However, the rule of the parameter selection still remains a problem. In this paper, we formulate the attack-defense model as an evolutionary game accordingly, and then present an optimal solution, which achieves security assurance along with minimum resource cost. We then analyze the stability of our evolutionary strategy theoretically. Simulation results are given to evaluate the performance of the proposed algorithm under low QoS channels and severe DoS attacks, which demonstrates that our proposed protocol canworks even in the extreme case."
Resource-Aware Photo Crowdsourcing Through Disruption Tolerant Networks.,"Photo crowdsourcing with smartphone has attracted considerable attention recently due to the prevalence of smartphones and the rich information provided by photos. In scenarios such as disaster recovery or battlefield, where the cellular network is partly damaged or severely overloaded, Disruption Tolerant Networks (DTNs) become the best way to deliver the crowdsourced photos. Since the bandwidth and storage resources in DTN are very limited and not enough to deliver all the crowdsourced photos, it is important to prioritize more valuable photos to use the limited resources. In this paper, we design a resource-aware photo crowdsourcing framework in DTN, which uses photo metadata including the smartphone's location, orientation, and other built-in camera's parameters, to estimate the value of photos. We propose a photo selection algorithm to maximize the value of photos delivered to the command center considering bandwidth and storage constraints. Both prototype implementation and trace-driven simulations demonstrate the effectiveness of our design."
Mayflower: Improving Distributed Filesystem Performance Through SDN/Filesystem Co-Design.,"In this paper, we introduce Mayflower, a new distributed filesystem that is co-designed from the ground up to work together with a network control plane. In addition to the standard distributed filesystem components, Mayflower has a flow monitor and manager running alongside a software-defined networking controller. This tight coupling with the network controller enables Mayflower to make intelligent replica selection and flow scheduling decisions based on both filesystem and network information. It further enables Mayflower to perform global optimizations that are unavailable to conventional distributed filesystems and network control planes. Our evaluation results from both simulations and a prototype implementation show that Mayflower reduces average read completion time by more than 25% compared to current state-of-the-art distributed filesystems with an independent network flow scheduler, and more than 75% compared to HDFS with ECMP."
A Parity-Based Data Outsourcing Model for Query Authentication and Correction.,"We propose a Parity-based Data Outsourcing(PDO) model in this paper. This model outsources a set of raw data by associating it with a set of parity data and then distributing both sets of data among a number of cloud servers that are managed independently by different service providers. Users query the servers for the data of their interest and are allowed to perform both authentication and correction. The former refers to the capability of verifying if the query result they receive is correct (i.e., all data items that satisfy the query condition are received, and every data item received is original from the data owner), whereas the latter, the capability of correcting the corrupted data, if any. A data item may be corrupted unintentionally (e.g, because of errors in systems and/or networking) or intentionally (e.g., by malicious service providers or because of systems being compromised by hackers). Existing techniques support only query authentication, but not error correction. Moreover, they all rely on complex cryptographic techniques and require the cloud server to build verification objects. In contrast, our approach achieves both without using any encryption. It does not require to install any additional software on a cloud server and thus can take advantage of the many cloud data management services available on the market today. We address the challenges of PDO implementation, including parity coding, database encoding, data retrieval, and database insertion and deletion, and evaluate the performance potential of PDO through analysis, simulation, and prototyping. Our results indicate its excellent performance in terms of storage, communication, and computation overhead."
Cure: Strong Semantics Meets High Availability and Low Latency.,"Developers of cloud-scale applications face a difficult decision of which kind of storage to use, summarised by the CAP theorem. Currently the choice is between classical CP databases, which provide strong guarantees but are slow, expensive, and unavailable under partition, and NoSQL-style AP databases, which are fast and available, but too hard to program against. We present an alternative: Cure provides the highest level of guarantees that remains compatible with availability. These guarantees include: causal consistency (no ordering anomalies), atomicity (consistent multi-key updates), and support for high-level data types (developer friendly API) with safe resolution of concurrent updates (guaranteeing convergence). These guarantees minimise the anomalies caused by parallelism and distribution, thus facilitating the development of applications. This paper presents the protocols for highly available transactions, and an experimental evaluation showing that Cure is able to achieve scalability similar to eventually-consistent NoSQL databases, while providing stronger guarantees."
Flexible Instance: Meeting Deadlines of Delay Tolerant Jobs in the Cloud with Dynamic Pricing.,"A wide range of cloud computing jobs are delay tolerant up to a predefined deadline. Existing IaaS services offer either high cost and high fulfillment ratio or low cost without fulfillment ratio guarantee, where the fulfillment ratio is the ratio of job execution time to the time between job submission and completion. Neither of the services represents a cost-effective way to exploit job elasticity. This work proposes flexible instance, a cloud service where user-specified service fulfillment ratio, as a new pricing factor, is guaranteed by the provider to meet deadlines. Job elasticity is exploited by the provider to enhance resource utilization, by regulating demand fluctuation through computation arbitrage across the temporal domain. We leverage a two-stage pricing framework to agilely adapt cloud resource price to the demand-supply dynamics. The first stage uses an online strategy to reserve resources for each cloud user to guarantee its specified fulfillment ratio. We set the price of resources dynamically according to resource utilization, with a pricing curve O(ln p)-competitive to the optimal fixed-price offline strategy in provider revenue. The second stage allows cloud users to submit a small budget to compete for extra service fulfillment ratio for execution speedup. A Nash bargaining framework is explored to achieve fairness, resource efficiency, and revenue maximization simultaneously. Extensive simulations driven by real-world traces show that flexible instance can reduce user cost for job execution while increasing provider revenue."
SpotLight: An Information Service for the Cloud.,"Infrastructure-as-a-Service cloud platforms are incredibly complex: they rent hundreds of different types of servers across multiple geographical regions under a wide range of contract types that offer varying tradeoffs between risk and cost. Unfortunately, the internal dynamics of cloud platforms are opaque along several dimensions. For example, while the risk of servers not being available when requested is critical in optimizing the cloud's risk-cost tradeoffs, it is not typically made visible to users. Thus, inspired by prior work on Internet bandwidth probing, we propose actively probing cloud platforms to explicitly learn such information, where each ""probe"" is a request for a particular type of server. We model the relationships between different contracts types to develop a market-based probing policy, which leverages the insight that real-time prices in cloud spot markets loosely correlate with the supply (and availability) of fixed-price on-demand servers. That is, the higher the spot price for a server, the more likely the corresponding fixed-price on-demand server is not available. We incorporate market-based probing into SpotLight, an information service that enables cloud applications to query this and other data, and use it to monitor the availability of more than 4500 distinct server types across 9 geographical regions in Amazon's Elastic Compute Cloud over a 3 month period. We analyze this data to reveal interesting observations about the platform's internal dynamics. We then show how SpotLight enables two recently proposed derivative cloud services to select a better mix of servers to host applications, which improves their availability from ~70-90% to near 100% in practice."
Routing and Scheduling of Social Influence Diffusion in Online Social Networks.,"Owing to the rising popularity of online social networking services (OSNs), studies on social influence and its diffusion have received significant attention from the research community. Prior research has mostly studied the impact of single-hop influence diffusion and multi-hop influence broadcast in online social networks. Very little research explores the idea of guiding (routing) multi-hop social influence towards a specific target. In this paper, we motivate the needs of timely routing social influence to formulate a new optimization problem, namely, Routing And Scheduling of Target-Oriented Social Influence Diffusion (RAS-TOSID). Accordingly, we propose the Efficient Routing And Scheduling with Social and Temporal decOmposition (ERASSTO) algorithm, which finds the optimal solution to RAS-TOSID in polynomial time. We carry out a user study by implementing ERASSTO in Facebook and conduct a comprehensive evaluation on ERASSTO and alternative approaches by simulation. The result shows that ERASSTO significantly outperforms other algorithms regarding solution quality and computational efficiency."
Social Graph Publishing with Privacy Guarantees.,"Online social network graphs provide useful insights on various social phenomena such as information dissemination and epidemiology. Unfortunately, social network providers often refuse to publish their social network graphs due to privacy concerns. Recently, differential privacy has become the widely accepted criteria for privacy preserving data publishing because it provides strongest privacy guarantees for publishing sensitive datasets. Although some work has been done on publishing matrices with differential privacy, they are computationally unpractical as they are not designed to handle large matrices such as the adjacency matrices of OSN graphs. In this paper, we propose a random matrix approach to OSN graph publishing, which achieves storage and computational efficiency by reducing the dimensions of adjacency matrices and achieves differential privacy by adding a small amount of noise. Our key idea is to first project each row of an adjacency matrix into a low dimensional space using random projection, and then perturb the projected matrix with random noise, and finally publish the perturbed and projected matrix. In this paper, we first prove that random projection plus random perturbation preserve differential privacy, and also that the random noise required to achieve differential privacy is small. We then validate the proposed approach and evaluate the utility of the published data for two different applications, namely node clustering and node ranking, using publicly available OSN graphs of Facebook, Live Journal, and Pokec."
ChitChat: An Effective Message Delivery Method in Sparse Pocket-Switched Networks.,"The ubiquitous adoption of portable smart devices has enabled a new way of communication via Pocket Switched Networks (PSN), whereby messages are routed by personal devices inside the pockets of ever-moving people. PSNs provide opportunities for various interesting applications such as location based social networking, geolocal advertising, and military missions in active battlefields where the central communication tower is unavailable. One key challenge of the successful roll-out of PSN applications is the difficulty of achieving high message delivery ratio due to the dynamic nature of moving people and spatial-temporal sparsity in such networks. In this paper, we propose a novel message routing approach, called ChitChat, which exploits users' direct and transient social interests via discriminatory gossiping to penetrate messages deeper into the network. Our approach enables message carriers to make opportunistic and distributed routing decisions based on the likelihood a potential message receiver will meet individuals that have a high chance to forward the message to the destination. Our experimental results have demonstrated that our approach achieves higher delivery ratios against the two more recent state-of-the-art algorithms, while maintaining a lower communication overhead against flooding and reducing the amount of time messages remain idlein buffers."
On Source Dependency Models for Reliable Social Sensing: Algorithms and Fundamental Error Bounds.,"This paper develops a simplified dependency model for sources on social networks that is shown to improve the quality of fact-finding -- assessing veracity of observations shared on social media. Recent literature developed a mathematical approach for exploiting social networks, such as Twitter, as noisy sensor networks that report observations on the state of the physical world. It was shown that the quality of state estimation from such noisy data, known as fact-finding, was a function of assumptions made regarding the independence of sources or lack thereof. When sources propagate information they hear from others (without verification), correlated errors may arise that degrade fact-finding performance. This work advances the state of the art by developing a simplified model of dependencies between sources and designing an improved dependency-aware estimator to assess veracity of observations, taking into account the observed dependency structure. A fundamental error bound is derived for this estimator to understand the gap in its performance from optimal. It is shown that the new estimator outperforms state of the art fact-finders and, in some cases, yields an accuracy close to the fundamental error bound."
Efficient Top-k Result Diversification for Mobile Sensor Data.,"Due to recent developments in sensor technologies, mobile sensor device use has become widespread, and many researchers have been attempting to leverage data collected by these devices. We call such data 'mobile sensor data'. Mobile sensor data are geo-referenced data with environmental attribute values, and they enable us to determine the geographical distribution of hot spots by retrieving data (such as higher air-pollution index values) with comparatively extreme environmental attribute values. Top-k search result diversification in geographical space is valid for applications of this sort. However, the preference scores for data items are different from each user's interest, and must be calculated for each query from scratch. In this case, the computational cost of a naive method is excessively high when the amount of mobile sensor data is very large. Thus, in this paper, we propose an efficient top-k search result diversification method for mobile sensor data. In a naive method, it is necessary to scan all data existing in a given query range when seeking the best data. Our proposed method, however, can reduce the amount of scanned data by exploiting cluster information, and the query result can thereby be returned much more rapidly. Moreover, a number of optimization problems can be solved using only one cluster file set. Experimental results show that our proposed method involves short computation time and reduces the disk IO cost in comparison with a naive method."
Energy Minimization for Quality-Constrained Video with Multipath TCP over Heterogeneous Wireless Networks.,"The advancements in wireless infrastructures and communication technologies prompt the bandwidth aggregation for mobile video delivery over heterogeneous access networks. Multipath TCP (MPTCP) is the transport protocol recommended by IETF to enable concurrent data transmissions over multiple communication paths. However, it still remains problematic to deliver user-satisfied video services with the existing MPTCP schemes due to the contradiction between mobile device energy and video distortion. To enable the energy-efficient and quality-guaranteed video streaming, this paper presents an Energy-Distortion Aware MPTCP (EDAM) solution. First, we develop an analytical model to capture the energy-distortion tradeoff for multipath video transmission over heterogeneous wireless networks. Second, we propose a video flow rate allocation algorithm to minimize the energy consumption while achieving target video quality based on utility maximization theory. The performance of the proposed EDAM is evaluated through extensive emulations in Exata involving real-time H.264 video streaming. Evaluation results demonstrate that EDAM outperforms the reference MPTCP schemes in reducing energy consumption, as well as in improving video PSNR (Peak Signal-to-Noise Ratio)."
Efficient Resource Allocation and Consolidation with Selfish Agents: An Adaptive Auction Approach.,"Through virtualization technologies, modern enterprises can build private clouds to support the daily operations of their subsidiaries and consolidate resources from them. In making these resource allocation and consolidation decisions, they want to maximize the achieved utilities and minimize the incurred costs by their subsidiaries, respectively. However, these subsidiaries very often operate autonomously and have private information about job characteristics and energy costs. Due to this information asymmetry, they might be motivated to behave in their own best interests rather than that of the enterprise. To solve these principal-agent problems, we design a tunable auction under which subsidiaries submit bids and resources are allocated in proportion to their bids. We show that the induced competition game obtains a unique Nash equilibrium, under which hidden characteristics of subsidiaries can be revealed. By using variational inequality techniques, we derive the dynamics of the Nash equilibrium as a function of the auction parameters. We design a feedback control mechanism to dynamically adjust the tunable auction parameters based on observable information such as the bids and the resulting allocations. We prove that our adaptive auction converges to the optimal Nash equilibrium under which the aggregate utility of an enterprise is maximized."
HIDE: AP-Assisted Broadcast Traffic Management to Save Smartphone Energy.,"WiFi is a major source of energy consumption on smartphones. Unfortunately, a non-negligible portion of the WiFi energy consumption is spent for frames that are useless to the smartphone. For example, energy is wasted to receive WiFi broadcast frames that are not needed by any smartphone application. What's worse, in order to process the broadcast frames received, a smartphone in suspend mode switches from suspend mode to high power active mode and stays there for a while. As such, additional energy is wasted to do the processing. In this paper, we design a system, namely HIDE, to reduce smartphone energy wasted on useless WiFi broadcast traffic. With our system, smartphones in suspend mode do not receive useless broadcast frames or wake up to process useless broadcast frames. Our trace-driven simulation shows that the HIDE system saves 34%-75% energy for Nexus One when 10% of the broadcast frames are useful to the smartphone. Our overhead analysis demonstrates that our system has negligible impact on network capacity and packet round-trip time."
Accurate Spatial Calibration of RFID Antennas via Spinning Tags.,"Recent years have witnessed the advance of RFID-based localization techniques that demonstrate high precision. Many efforts have been made locating RFID tags with a mandatory assumption that the RFID reader's position is known in advance. Unfortunately, calibrating reader's location manually is always time-consuming and laborious in practice. In this paper, we present Tagspin, an approach using COTS tags to pinpoint the reader (antenna) quickly and easily with high accuracy. Tagspin enables each tag to emulate a circular antenna array by uniformly spinning on the edge of a rotating disk. We design an SAR-based method for estimating the angle spectrum of the target reader. Compared to previous AoA-based techniques, we employ an enhanced power profile modeling the signal power received from the reader along different spatial directions, which is more accurate and immune to ambient noise as well as measurement errors caused by hardware characteristics. Besides, we find that tag's phase measurements in practice are related to its orientation. To the best of our knowledge, we are the first to point out this fact and quantify the relationship between them. By calibrating the phase shifts caused by orientation, the positioning accuracy can be improved by 3.7. We have implemented Tagspin withCOTS RFID devices and evaluated it extensively. Experimentalresults show that Tagspin achieves mean accuracy of 7.3cm with standard deviation of 1.8cm in 3D space."
WiLocator: WiFi-Sensing Based Real-Time Bus Tracking and Arrival Time Prediction in Urban Environments.,"Offering the services of real-time tracking and arrival time prediction is a common welfare for bus riders and transit agencies, especially in urban environments. On the down side, the traditional GPS-based solutions work poorly in urban areas due to urban canyons, while the location systems based on cellular signal also suffer from inherent limitations. In this paper, we present a powerful tool named Signal Voronoi Diagram (SVD) to partition the radio-frequency (RF) signal space of WiFi Access Points (APs), distributed where a bus travels, into Signal Cells, and then into fine-grained Signal Tiles, tackling the problem of noisy received signal strength (RSS) readings and possible AP dynamics. On top of SVD, we present a novel framework so-called WiLocator, to track and predict the arrival time of an urban bus based on the surrounding WiFi information collected by the commodity off-the-shelf (COTS) smartphones of bus riders, the mobility constraint of a bus and the temporal consistency of travel time of buses on the overlapped road segments. We also show the WiLocator's power of generating an accurate and real-time traffic map with the predicted travel time on each road segment. We implement the prototype of WiLocator and conduct the in-situ experiment to demonstrate its accuracy."
CACE: Exploiting Behavioral Interactions for Improved Activity Recognition in Multi-inhabitant Smart Homes.,"We propose CACE (Constraints And Correlations mining Engine) which investigates the challenges of improving the recognition of complex daily activities in multi-inhabitant smart homes, by better exploiting the spatiotemporal relationships across the activities of different individuals. We first propose and develop a loosely-coupled Hierarchical Dynamic Bayesian Network (HDBN), which both (a) captures the hierarchical inference of complex (macro-activity) contexts from lower-layer microactivity context (postural and improved oral gestural context), and (b) embeds the various types of behavioral correlations and constraints (at both micro-and macro-activity contexts) across the individuals. While this model is rich in terms of accuracy, it is computationally prohibitive, due to the explosive increase in the number of jointly-defined states. To tackle this challenge, we employ data mining to learn behaviorally-driven context correlations in the form of association rules, we then use such rules to prune the state space dramatically. To evaluate our framework, we build a customized smart home system and collected naturalistic multi-inhabitant smart home activities data. The system performance is illustrated with results from real-time system deployment experiences in a smart home environment reveals a radical (max 16 fold) reduction in the computational overhead compared to traditional hybrid classification approaches, as well as an improved activity recognition accuracy of max 95%."
Overlay Design for Topic-Based Publish/Subscribe under Node Degree Constraints.,"It is important to build overlays for topic-based publish/subscribe (pub/sub) under resource constraints. In a topic-connected overlay (TCO), each topic t induces a connected sub-overlay among all nodes interested in t. Existing work merely consider how to optimize a complete TCO and implicitly commit the unrealistic assumption of unlimited resources. In contrast, we make maximum use of restricted node degree budgets to build a partial TCO. We formalize the notion of TCO support to capture the quality of the pub/sub overlay. Furthermore, we demonstrate that partial TCOs usually exhibit significantly better cost-effectiveness in practice. We propose two problems of maximizing TCO support in a partial TCO: (1) PTCOA with a bounded average node degree and (2) PTCOM under the maximum node degree constraint. We design two greedy algorithms, which achieve the constant approximation ratios of (1-e
<sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">-1</sup>
) for PTCOA and (1-e
<sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">-1/6</sup>
) for PTCOM, respectively. Empirical evaluation demonstrates the scalability of our algorithms under a variety of pub/sub workloads. Given practical data sets extracted from Facebook and Twitter, our algorithms produce an 80% TCO with fewer than 20% of the node degree budget as a complete TCO. We also show experimentally that it is promising to design decentralized protocols to compute a partial TCO for pub/sub."
Service Placement for Detecting and Localizing Failures Using End-to-End Observations.,"We consider the problem of placing services in a telecommunication network in the presence of failures. In contrast to existing service placement algorithms that focus on optimizing the quality of service (QoS), we consider the performance of monitoring failures from end-to-end connection states between clients and servers, and investigate service placement algorithms that optimize the monitoring performance subject to QoS constraints. Based on novel performance measures capturing the coverage, the identifiability, and the distinguishability in monitoring failures, we formulate the service placement problem as a set of combinatorial optimizations with these measures as objective functions. In particular, we show that maximizing the distinguishability is equivalent to minimizing the uncertainty in failure localization. We prove that all these optimizations are NP-hard. However, we show that the objectives of coverage and distinguishability have a desirable property that allows them to be approximated to a constant factor by a greedy algorithm. We further show that while the identifiability objective does not have this property, it can be approximated by the maximumdistinguishability placement in the high-identifiability regime. Our evaluations based on real network topologies verify the effectiveness of the proposed algorithms in improving the monitoring performance compared with QoS-based service placement."
Live Exploration of Dynamic Rings.,"Almost all the vast literature on graph exploration assumes that the graph is static: its topology does not change during the exploration, except for occasional faults. To date, very little is known on exploration of dynamic graphs, where the topology is continously changing. The few studies have been limited to the centralized (or post-mortem) case, assuming complete a priori knowledge of the changes and the times of their occurrence, and have only considered fully synchronous systems. In this paper, we start the study of the decentralized (or live) exploration of dynamic graphs, i.e. when the agents operate in the graph unaware of the location and timing of the changes. We consider dynamic rings under the standard 1-interval-connected restriction, and investigate the feasibility of their exploration, in both the fully synchronous and semi-synchronous cases. When exploration is possible we examine at what cost, focusing on the minimum number of agents capable of exploring the ring. We establish several results highlighting the impact that anonymity and structural knowledge have on the feasibility and complexity of the problem."
Randomized Load Balancing in Finite Regimes.,"Randomized load balancing is a cost efficient policy for jobscheduling in parallel server queueing systems whereby, with everyincoming job, a central dispatcher randomly polls some servers andselects the one with the smallest queue. By exactly deriving thejobs' delay distribution in such systems, in explicit and closedform, Mitzenmacher [13] proved the so-called 'power-of-two' result, which states that the random polling of only two serversyields an exponential improvement in delay over randomly selectinga single server. Such a fundamental result, however, was obtainedin an asymptotic regime in the total number of servers, and doesdo not necessarily provide accurate estimates for practical finiteregimes with small or moderate number of servers. In this paper weobtain stochastic lower and upper bounds on the jobs' averagedelay in non-asymptotic/finite regimes, by extending ideas foranalyzing the particular case of the Join-the-Shortest-Queue (JSQ) policy. Numerical illustrations indicate not only that the (lower) bounds are remarkably accurate, but also that the asymptoticapproximation can be misleading in scenarios with a small numberof servers, and especially at very high utilizations."
Spectrum Matching.,"Dynamic spectrum access (DSA) redistributes spectrum from service providers with spare channels to those in need for them. Existing works on such spectrum exchange mainly focus on double auctions, where an auctioneer centrally enforces a certain spectrum allocation policy. In this paper, we take a different and new perspective, proposing to use matching as an alternative tool to realize DSA in a distributed way for a free market, which consists of only buyers and sellers, but no trustworthy third-party authority. Compared with conventional many-to-one matching problems, the spectrum matching problem is distinctively challenging due to the interference bound between buyers: the same channel can be reused by an unlimited number of non-interfering buyers, but must be exclusively occupied by only one of interfering buyers. In this paper, we firstly formulate the spectrum matching problem as a many-to-one matching with peer effects, i.e., a buyer's utility is affected by other buyers who are matched to the same seller. We then present a two-stage distributed algorithm that converges to an interference-free and Nash-stable matching result. Simulations show that the proposed distributed matching algorithm can achieve 90% of the social welfare from the optimal matching result."
CACA: Link-Based Channel Allocation Exploiting Capture Effect for Channel Reuse in Wireless Sensor Networks.,"In this paper, we exploit the capture-effect for channel allocation. We experimentally show the characteristics of capture-effect across different channels, over time, and in different network densities. Then, we introduce CACA, an effective channel assignment protocol for wireless sensor networks. Traditional channel assignment protocols utilize all available channels to minimize interferences between any adjacent links. However, their performances are often not much better than the case of using single channel only. This is mainly due to an assumption that all channels are independent and quality of all channels are similar. However, this is a false assumption. In fact, there are only a few channels that show very good quality at any given time. The CACA avoids this problem by utilizing a few good channels and reuse these channels. When the channels are reused it relies on the capture-effect to ensure at least one of the contending nodes to transmit successfully. Maximizing this capture probability is a main objective of the CACA whenever the channels need to be reused. We evaluate the CACA on a 140-node wireless sensor network testbed and compare its performance with another benchmark protocol. Our results indicate that the CACA can improve the packet reception ratio of every link. As a result of this improvement, end-to-end throughput increases upto 100% in the case of a wireless sensor network with bursty traffic."
An Analysis of Onion-Based Anonymous Routing for Delay Tolerant Networks.,"Delay tolerant network (DTN) routing provides a communication primitive in intermittently disconnected networks, such as battlefield communications and human-contact networks. In these applications, the anonymity preserving mechanism, which hides the identities of communicating parties, plays an important role as a defense against cyber and physical attacks. While anonymous routing protocols for DTNs have been proposed in the past, to the best of our knowledge, there is no work that emphasizes the theoretical aspects. In this paper, we first design an abstract of anonymous routing protocols for DTNs and augment the existing solution with multi-copy message forwarding. Then, we construct simplified mathematical models, which can be used to understand the fundamental performance and security guarantees of onion-based anonymous routing in DTNs. The numerical and simulation results using randomly generated contact graphs and the real traces demonstrate that our models provide very close approximations to the performance of the anonymous DTN routing protocol."
Directional Beam Alignment for Millimeter Wave Cellular Systems.,"Transmission in millimeter wave (mmW) band has a big potential to provide orders of higher wireless bandwidth. To combat the high channel loss in high frequency band, beamforming is generally taken to transmit along the direction that provides the maximum transmission gain. This requires the MAC protocol to facilitate the finding of the optimal beamforming direction. Exiting protocol suggests the rotational channel measurement which may introduce high measurement cost, and compromise the transmission capacity. This paper presents a comprehensive design for more efficient directional beam alignment in mmW cellular networks. Instead of exhaustively searching all possible beamforming directions at the transmitter (TX) and the receiver (RX), our proposed scheme selects only a fairly small number of TX and RX beam pairs to facilitate effective beam alignment. To avoid long and resource-consuming exhaustive search, our scheme not only takes advantage of the low rank characteristics of the channel to estimate the full channel information with a small number of measurements, but also further exploits the channel estimation from initial measurements to guide the selection of future beam pairs for more effective measurements later. These strategies help to speed up the process of finding satisfactory beam pairs. We perform extensive simulations to evaluate the performance of our proposed schemes, and our results demonstrate our scheme can significantly outperform other schemes in terms of measurement effectiveness and cost efficiency."
"Measurement, Modeling, and Analysis of TCP in High-Speed Mobility Scenarios.","The rapid growth of high-speed transit systems, such as High Speed Rail (HSR), is putting considerable pressure on TCP-based data transmission. It is well known that TCP is suffering from severe throughput degradation in high-speed mobility scenarios. The root cause at the transport layer however remains unclear and largely undetermined to date. In this paper, we aim to pinpoint the throughput bottlenecks and develop a throughput model to understand TCP in high-speed mobility environments. Based on the analysis of real-world HSR traces, we find that high-speed mobility will introduce significant challenges to the packet retransmission process after timeouts. And ACKs are more likely to trigger spurious retransmission timeouts in TCP flows in high-speed mobile environments. Such problems are not yet considered in the existing TCP models because classic timeouts can easily be recovered by retransmission in stationary scenarios. We therefore propose an enhanced TCP throughput model to integrate the above features. Our model analysis indicates that the optimization of TCP ACK latency is critical to obtain better throughput. Moreover, reliable retransmission mechanisms, e.g., multi-path TCP (MPTCP), can also bring notable benefits in high-speed mobility environments."
Backlog-Aware SRPT Flow Scheduling in Data Center Networks.,"The rapidly developing soft real-time data center applications impose stringent delay requirements on internal data transfers. Therefore many recently emerged network protocols in data center share a common goal of decreasing Flow Completion Time (FCT), in which case the Shortest Remaining Processing Time (SRPT) scheduling discipline has attracted widespread attentions. However, SRPT suffers the instability issue, incurring more and more flows left uncompleted even when traffic load is within network capacity, which implies unnecessary bandwidth waste. To solve the problem, this paper proposes a backlog aware scheduling algorithm (BASRPT) that stabilizes queue length while maintaining relatively low FCT based on Lyapunov optimization. To overcome the huge computational overhead, a fast and practical approximation algorithm called fast BASRPT is also developed. Extensive flow-level simulations show that fast BASRPT indeed stabilizes switch queue and obtains a higher throughput while being able to push FCT arbitrarily close to the optimal value in the condition of feasible traffic load."
An NFV Orchestration Framework for Interference-Free Policy Enforcement.,"Network functions virtualization is a new paradigm to offer flexibility of software network function processing on demand. Policy enforcement satisfies network function policies that requires flows to traverse through given sequences of network functions. We summarize three desired properties of virtual network function placement, namely policy enforcement, interference freedom, and resource isolation. However, none of existing solutions can satisfy all of them. In this paper, we present a novel SDN-based NFV orchestration framework, called APPLE, to enforce network function policies while providing the above properties. We present detailed design considerations and prototype implementation. We conduct experiments using representative network topologies, traffic matrices, and policy chains. The results from both prototype experiments and simulations show that APPLE is resource efficient and can quickly react to traffic changes."
Bandwidth-Greedy Hashing for Massive-Scale Concurrent Flows.,"The explosion of network bandwidth poses greatchallenges to data-plane flow processing. Due to the variable andpoor worst-case performance, naive hash table is incapable ofwire-speed processing. State-of-the-art schemes rely on multiplehash functions for enhanced load balancing to improve the worst-case performance. These schemes exploit the memory hierarchyand allocate compact on-chip data structures as the off-chiphash table summaries. However, when the flow number inflates, they fail to scale the on-chip memory consumption gracefully. This work is inspired by modern DRAM's burst-transfer feature. Specifically, we propose bandwidth-greedy hashing which resolveshash collisions with just one DRAM burst. Besides, load balancingefforts are made in an ""on-demand"" fashion. This radicaldesign surmounts the major obstacle of mapping multiple choicehashing schemes to real-world hierarchical memory systems formassive-scale items. Essentially, this solution follows a designpattern of on-demand load balancing and can be regarded asa generalization of closed hashing. To establish its theoreticalbase, we analyze it via Poisson distribution approximation. Theevaluation on DRAMSim2 reports that our scheme requires onlyone DRAM burst access (in 99.999% cases) and minuscule on-chip memory (less than 16MB, or 1% of the previous) to supportlookups for 100M flows at a throughput of 122.82Mpps."
Fast Total Ordering for Modern Data Centers.,"The performance profile of local area networks has changed over the last decade, but many practical group communication and ordered messaging tools rely on core ideas invented over a decade ago. We present the Accelerated Ring protocol, a novel ordering protocol that improves on the performance of standard token-based protocols by allowing processes to pass the token before they have finished multicasting. This performance improvement is obtained while maintaining the correctness and other beneficial properties of token-based protocols. On 1-gigabit networks, a single-threaded daemon-based implementation of the protocol reaches network saturation, and can reduce latency by 45% compared to a standard token-based protocol while simultaneously increasing throughput by 30%. On 10-gigabit networks, the implementation reaches throughputs of 6 Gbps, and can reduce latency by 30-35% while simultaneously increasing throughput by 25-40%. A production implementation of the Accelerated Ring protocol has been adopted as the default ordering protocol for data center environments in Spread, a widely-used open-source group communication system."
A De-compositional Approach to Regular Expression Matching for Network Security Applications.,"Regular expressions are a very common tool for network security applications because they can match precisely and maintain high matching speed even for many simultaneous patterns. Their core feature is efficient representation as an automaton, where much of the interaction between patterns can be pre-computed and aggregated. Many algorithms have been devised to try and improve this pre-computation to not take exponential space while keeping high performance, but none has met all the requirements of fast, automated construction, small memory image, and high matching speed. We present Match Filtering, a technique for de-composing regular expressions into segments that can be matched independently, while a stateful post-processing engine filters these matches to eliminate those that do not correspond to matches of the original regular expression. Using standard CPU instructions, the post-processing engine can more efficiently represent constructs that would require a multiplicative increase in automaton states. Because the pre-processing is simple, automaton construction can be automated and fast, and because most on-line processing is done by a DFA, its matching speed is close to that of a DFA alone. We demonstrate experimentally 30 smaller, fast (seconds, not minutes) automaton construction and 43% faster matching speeds than state-of-the-art software algorithms."
Privacy-Preserving Data Classification and Similarity Evaluation for Distributed Systems.,"Data classification is a widely used data mining technique for big data analysis. By training massive data collected from the real world, data classification helps learners discover hidden data patterns. In addition to data training, given a trained model from collected data, a user can classify whether a new incoming data belongs to an existing class, or, multiple distributed entities may collaborate to test the similarity of their trained results. However, due to data locality and privacy concerns, it is infeasible for large-scale distributed systems to share each individual's datasets with each other for data similarity check. On the one hand, the trained model is an entity's private asset and may leak private information, which should be well protected from all other non-collaborative entities. On the other hand, the new incoming data may contain sensitive information which cannot be disclosed directly for classification. To address the above privacy issues, we propose a privacy-preserving data classification and similarity evaluation scheme for distributed systems. With our scheme, neither new arriving data nor trained models are directly revealed during the classification and similarity evaluation procedures. The proposed scheme can be applied to many fields using data classification and evaluation. Based on extensive real-world experiments, we have also evaluated the privacy preservation, feasibility, and efficiency of the proposed scheme."
Secure Surfing: Privacy-Preserving Speeded-Up Robust Feature Extractor.,"Large-scale multimedia data are being exponentially generated, stored and processed continuously nowadays. Along with the data explosion, the data owner is highly motivated to outsource his/her huge amount of data storage and computation-expensive processing jobs to the cloud by leveraging its abundant resources for cost reduction and flexibility. Despite the fascinating advantages, security and privacy concerns are the primary obstacles that prevent the wide adoption of this promising information technology paradigm. In this work, we aim at outsourcing Speeded-up Robust Features (SURF), a widely-used feature extraction algorithm, to the intrinsically untrusted cloud, while protecting data owner's private information on the outsourced image data. We, for the first time, propose a practical privacy-preserving outsoucing scheme for SURF that can preserve its key characteristics in terms of distinctiveness and robustness. By randomly splitting the original image data and distributing the encrypted data shares to two independent cloud servers, we first design two novel efficient protocols for secure multiplication and comparison by leveraging somewhat homomorphic encryption (SHE) and single-instruction multiple-data (SIMD). We then carefully tune every step of the original SURF to adapt it to the ciphertext domain. A thorough theoretical analysis of effectiveness and security shows that our scheme is practically secure and approximates well the performance of the original SURF executed in the plaintext domain. Extensive experiments are also conducted over real-world image datasets to show that our scheme outperforms the existing solution and indeed performs comparably to the original SURF in preserving its various characteristics including image scale invariance, rotation invariance and robust matching across 3D viewpoint change etc."
"D-DEMOS: A Distributed, End-to-End Verifiable, Internet Voting System.","E-voting systems have emerged as a powerful technology for improving democracy by reducing election cost, increasing voter participation, and even allowing voters to directly verify the entire election procedure. Prior internet voting systems have single points of failure, which may result in the compromise of availability, voter secrecy, or integrity of the election results. In this paper, we present the design, implementation, security analysis, and evaluation of D-DEMOS, a complete e-voting system that is distributed, privacy-preserving and end-to-end verifiable. Our system includes a fully asynchronous vote collection subsystem that provides immediate assurance to the voter her vote was recorded as cast, without requiring cryptographic operations on behalf of the voter. We also include a distributed, replicated and fault-tolerant Bulletin Board component, that stores all necessary election-related information, and allows any party to read and verify the complete election process. Finally, we also incorporate trustees, i.e., individuals who control election result production while guaranteeing privacy and end-to-end-verifiability as long as their strong majority is honest. Our system is the first e-voting system whose voting operation is human verifiable, i.e., a voter can vote over the web, even when her web client stack is potentially unsafe, without sacrificing her privacy, and still be assured her vote was recorded as cast. Additionally, a voter can outsource election auditing to third parties, still without sacrificing privacy. Finally, as the number of auditors increases, the probability of election fraud going undetected is diminished exponentially. We provide a model and security analysis of the system. We implement a prototype of the complete system, we measure its performance experimentally, and we demonstrate its ability to handle large-scale elections."
Deadline-Sensitive User Recruitment for Probabilistically Collaborative Mobile Crowdsensing.,"Mobile crowdsensing is a new paradigm in which a group of mobile users exploit their carried smart devices to cooperatively perform a large-scale sensing job over urban environments. In this paper, we focus on the Deadline-sensitive User Recruitment (DUR) problem for probabilistically collaborative mobile crowdsensing, in which mobile users perform sensing tasks with certain probabilities, and multiple users might be recruited to cooperatively perform a common task, ensuring that the expected completion time be no larger than a deadline. In order to solve this problem, we propose a greedy approximation algorithm, which can achieve the logarithmic approximation ratio."
RF-ISee: Identify and Distinguish Multiple RFID Tagged Objects in Augmented Reality Systems.,"In this paper, we leverage RFID technology to label different objects with RFID tags, so as to realize the vision of ""show me what I see from the augmented reality system"". We deploy additional RFID antennas to the COTS depth camera and propose a continuous scanning-based scheme to scan the objects, i.e., the system continuously rotates and samples the depth of field and RF-signals from these tagged objects. In this way, we can accurately identify and distinguish multiple tagged objects, by pairing the tags with the objects according to the correlations between the depth of field and RF-signals. Our solution achieves an average match ratio of 91% in distinguishing up to dozens of tagged objects with a high deployment density."
OmniFlow: Coupling Load Balancing with Flow Control in Datacenter Networks.,"In this paper, we propose OmniFlow, a novel transport protocol which combines load balancing and flow control at the transport layer to optimize datacenter transfers. OmniFlow outweighs previous solutions in two aspects. Firstly, it can simultaneously and precisely measure the queueing latencies on multiple paths between two hosts. Secondly, OmniFlow adaptively integrates the load balancing and flow control modules and shares the same congestion metrics (i.e. queueing latencies) between them. Based on different network conditions, it either dynamically reroutes flows to utilize the bisection bandwidth or proactively adjusts flow rates to bound queueing occupancies. Our results show that OmniFlow can provide both low latency for small flows without sacrificing the throughput of elephant flows."
Approximate Agreement under Mobile Byzantine Faults.,This paper considers the Approximate Agreement problem in presence of mobile Byzantine agents. We prove lower bounds on the number of correct processes to solve such problem. To do that we prove that the existing solutions tolerant to Byzantine agents still holds in such case and under which conditions.
Preserving Incumbent Users' Privacy in Server-Driven Dynamic Spectrum Access Systems.,"Dynamic spectrum access (DSA) technique has emerged as a fundamental approach in improving spectrum utilization to mitigate the spectrum scarcity problem. As a key form of DSA, government is proposing to release more federal spectrum for sharing with commercial wireless users. However, the flourish of federal-commercial sharing hinges upon how federal privacy issues are managed. In current DSA proposals, the sensitive operation information of federal incumbent users (IUs) needs to be shared with a dynamic spectrum access system (SAS) to realize spectrum allocation. However, SAS is not necessarily trust-worthy for holding such sensitive IU data, especially considering that FCC allows some industry third parties (e.g., Google) to operate SAS for better efficiency and scalability. Therefore, the current proposals dissatisfy the IUs' privacy requirement. To address the privacy issues, this paper presents an IU-privacy-preserving SAS (IP-SAS) design, which realizes the spectrum allocation process through secure computation over ciphertext based on homomorphic encryption so that none of the IU operation information is exposed to SAS."
On MinMax-Memory Claims for Scientific Workflows in the In-memory Cloud Computing.,"We propose a new concept of minmax memory claim (MMC) to achieve cost-effective workflow computations in in-memory cloud computing environments. The minmax-memory claim is defined as the minimum amount of memory required to finish the workflow without compromising its maximum concurrency. With MMC, the workflow tenants can achieve the best performance via the maximum concurrency while minimizing the cost to use the memory resources. In this paper, we present the algorithms to find the MMC for workflow computation and evaluate its value by applying it to deadlock avoidance algorithms."
On the Efficiency of Decentralized Search in Expert Networks.,"Expert networks are formed by a group of expert-professionals with different specialties to collaboratively resolve specific queries posted to the network. In expert networks, decentralized search, operating purely on each expert's local information without any knowledge of network global structure, represents the most basic and scalable routing mechanism. However, there is still a lack of fundamental understanding of the efficiency of decentralized search. In this regard, we investigate decentralized search by quantifying its performance under a variety of network settings. Our key findings reveal that under certain network conditions, decentralized search can achieve significantly small query routing steps (i.e., between O(log n) and O(log
<sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">2</sup>
 n), n: total number of experts in the network). To the best of our knowledge, this is the first work studying fundamental behaviors of decentralized search in expert networks."
Virtual Machine Level Temperature Profiling and Prediction in Cloud Datacenters.,"Temperature prediction can enhance datacenter thermal management towards minimizing cooling power draw. Traditional approaches achieve this through analyzing task-temperature profiles or resistor-capacitor circuit models to predict CPU temperature. However, they are unable to capture task resource heterogeneity within multi-tenant environments and make predictions under dynamic scenarios such as virtual machine migration, which is one of the main characteristics of Cloud computing. This paper proposes virtual machine level temperature prediction in Cloud datacenters. Experiments show that the mean squared error of stable CPU temperature prediction is within 1.10, and dynamic CPU temperature prediction can achieve 1.60 in most scenarios."
Strategic Security Resource Allocation for Internet of Things.,"In many Internet of Thing (IoT) application domains security is a critical requirement, because malicious parties can undermine the effectiveness of IoT-based systems by compromising single components and/or communication channels. Thus, a security infrastructure is needed to ensure the proper functioning of such systems even under attack. In this paper, we focus on the problem of efficiently and effectively securing IoT networks by carefully allocating security tools."
FSQCN: Fast and Simple Quantized Congestion Notification in Data Center Ethernet.,"Currently, Quantized Congestion Notification (QCN) has been accepted as the standard layer 2 congestion control protocol in Data Center Ethernet. Although the good performance of QCN has been validated in many experiments, we find that QCN has two drawbacks. First, the incomplete binary search in QCN fails to find the proper sending rate, leading to complicate supplement mechanisms. Second, in face of unknown network environment, the rate setting of QCN is inconsistent. Consequently, QCN spends much time on acquiring the spare bandwidth. To address theses problems, we propose the Fast and Simple QCN (FSQCN), following the same framework as QCN. FSQCN complements the binary search and removes other complicate supplement mechanisms in QCN. Thus, FSQCN is much simpler than QCN. Moreover, FSQCN resets the sending rate to the link rate explicitly when the switch detects spare bandwidth. Extensive simulations validate that FSQCN controls the queue length well like QCN and responds faster than QCN."
Practical Concurrent Wireless Charging Scheduling for Sensor Networks.,"In complex terrain where mobile chargers hardly move around, a feasible solution to charge wireless sensor networks (WSNs) is using multiple fixed chargers to charge WSNs concurrently with relative long distance. Due to the radio interference in the concurrent charging, it is needed to schedule the chargers so as to facilitate each sensor node to harvest sufficient energy quickly. The challenge lies that each charger's charging utility cannot be calculated (or even defined) independently due to the nonlinear superposition charging effect caused by the radio interference. In this paper, we model the concurrent radio charging, and formulate the concurrent charging scheduling problem (CCSP) whose objective is to design a scheduling algorithm for the chargers so as to minimize the time spent on charging each sensor node with at least energy E. We prove that CCSP is NP-hard, and propose a greedy algorithm based on submodular set cover problem. We also propose a genetic algorithm for CCSP. Simulation results show that the performance of the greedy CCSP algorithm is comparable to that of the genetic algorithm."
A Geometric Windowing Algorithm in Network Data-Plane Verification.,"Network verification is attracting attention as a key technology to detect configuration errors before deploying the network. In verification, a set of packets to be inspected is usually specified by a window -- a multi-dimensional rectangle defined by packet header fields (e.g., address prefixes and port ranges). Network operators have to know the forwarding behaviors of packets inside the window, this can be regarded as the windowing query problem in computation geometry. This paper proposes a novel windowing algorithm for network verification. Unlike existing windowing algorithms, our algorithm runs on a compressed data structure, because the search space has to be represented in a compressed form due to the space complexity."
Dslash: Managing Data in Overloaded Batch Streaming Systems.,"Peak loads are a challenge for streaming systems. Here we present Dslash, a latency-driven controller that keeps and processes as much data as the system resources allow in order to meet a target latency."
Distributed Online Data Aggregation in Dynamic Graphs.,"We consider the problem of aggregating data in a dynamic graph, that is, aggregating the data that originates from all nodes in the graph to a specific node, the sink. In our model, nodes are endowed with unlimited memory and unlimited computational power. Yet, we assume that communications between nodes are carried out with pairwise interactions, where nodes can exchange control information before deciding whether they transmit their data or not, given that each node is allowed to transmit its data at most once. When a node receives a data from a neighbor, the node may aggregate it with its own data. We are interested in giving lower bounds for this problem, under two possible adversaries: the oblivious adversary, and the randomized adversary that chooses the pairwise interactions uniformly at random. For the online adaptive and the oblivious adversary, we give impossibility results when nodes have no knowledge about the graph and are not aware of the future. For the randomized adversary, we propose two optimal algorithms, (i) when nodes have no knowledge at all and (ii) when each node knows its future pairwise interactions with the sink."
Efficient Free-Rider Detection Using Symmetric Overlays.,"Edge-computing is one of the most promising techniques to leverage the excess capacity that exists at users' premises. Unfortunately, edge-computing may be vulnerable to free-riding, i.e., to nodes that attempt to benefit from the system without providing any service in return. Traditional approaches model free-riders as rational nodes that strive to maximize a utility and apply Game Theory concepts to devise mechanisms that deny any utility gain to nodes that deviate from the protocol. These mechanisms impose significant overheads. This paper proposes a new approach that avoids these overheads, which applies concepts of evolutionary game theory. We propose to devise lightweight mechanisms targeted for the optimistic setting where nodes adopt one of a small number of behaviours. If a small fraction of nodes follows alternative behaviours, then the lightweight mechanism should limit the utility gain of these nodesto control the increase in the number of these nodes. This allows altruistic nodes to detect their presence in time to switch to more robust mechanisms before the system reaches a state where the lightweight mechanisms can no longer cope with the existing behaviours. We apply this approach in the context of edge-assisted streaming and propose the use of carefully crafted symmetric overlays to support message dissemination in an environment with free-riders and white-washers. The topology maintenance procedures of our overlay encourage nodes to maintain stable symmetric links. Leveraging the topological properties of the resulting symmetric overlay, direct reciprocity mechanisms deny any utility gain to free-riders and white-washers, while limiting the utility gain of small fractions of nodes that adopt more sophisticated behaviours."
Distributed Encoding for Multiple-Inherited Locators to Accommodate Billions of Objects in the Internet.,"As the Internet of Things technologies evolve, billions of smart devices will be connected to the Internet. Therefore, the accelerated growth of users, applications and devices pose a great demand on the scalability of the Internet. In this paper, we develop a new Internet architecture -- Multiple-inherited Locators (MiL) -- to meet the future demand on addressing. MiL is based on Locator/ID addressing and hierarchical address allocation. The benefit of this new addressing scheme is the improved scalability of Internet routing by enhancing the prefix aggregation of locators. The number of locators which are needed grows with the Internet scale. In order to represent each locator as a unique binary representation by as few bits as possible, we develop a distributed encoding method to efficiently encode locators. As the Internet topology evolves, the distributed encoding method renumbering locators' codeword for the optimal encoding performance, and meanwhile endeavors to keep their codewords unchanged as much as possible. Because, we want to represent locators by fewer bits as well as keep addressing stable. We adopt an Encoding Table Adjustment algorithm to find a ""sweet spot"" that balances these two goals. According to our experiment, within a 2% increase on the expected locator length, our algorithm reduces the cost of renumbering locator by 99.9%."
Sprout: A Functional Caching Approach to Minimize Service Latency in Erasure-Coded Storage.,"The rapid growth of data traffic in storage systems has put a significant burden on the underlying networks of cloud storage systems. Historically, a key solution to relieve this traffic burden is caching [1]. Many companies have adopted erasure-coded storage systems. However, caching for data centers when the files are encoded with an erasure code has not been studied to the best of our knowledge. This paper proposes a new functional caching approach called Sprout that can efficiently capitalize on existing file coding in erasure-coded storage systems. In contrast to exact caching that stores chunks identical to original copies, our functional caching approach forms d new data chunks, which together with the existing n chunks satisfy the property of being an (n + d, k) MDS code. Thus, the file can now be recovered from any out of n + d chunks (rather than k out of n under exact caching), effectively extending coding redundancy, as well system diversity for scheduling file access requests. The proposed functional caching approach saves latency due to more flexibility to obtain k-d chunks from the storage system a very minimal additional computational cost of creating the coded cached chunks. While quantifying service latency erasure-coded storage systems is an open problem, we generalize previous results on probabilistic scheduling policy [2,3] that distributes file requests to cache and storage nodes with optimized probabilities, and derive a closed-form upper bound on mean service latency for the proposed functional caching approach."
UDS: A Unified Approach to Deterministic Multithreading.,"There are several applications for the deterministic execution of software, e.g. replicated systems, test and febugging scenarios. In all cases the execution should lead zo the same effects in order to be consistent. Multi-threading is one of the major sources of nondeterminism. Several deterministic scheduling algorithms exist that allow concurrent but deterministic executions despite of arbitrary switching decisions of the underlying system schedulers. We present the novel and flexible Unified Deterministic Scheduling algorithm (UDS) for weakly and fully deterministic systems. Compared to existing algorithms, UDS has a broad parameter set that can be (re-)configured even at runtime. Further, we show that many existing algorithms are merely a particular configuration of UDS."
DMZtore: A dispersed Data Storage System with Decentralized Multi-factor Access Control (Demo).,"While many commercial systems as well as academic techniques for data outsourcing to and content confidentiality from untrusted data stores have been developed over the last decade, when it comes to multi-factor authentication based layered security, existing approaches typically rely on a logically centralized service. In this demo, we present DMZtore edge storage system that incorporates a decentralized multi-factor access control scheme [1] achieving layered security."
Milk Carton: A Face Recognition-Based FTR System Using Opportunistic Clustered Computing.,"Family Tracing and Reunification (FTR) is the process whereby families separated by disasters are reunited. Current FTR systems use either inefficient paper-based forms and notice boards or digital registries that need the Internet, which may be unavailable during disasters. In this demonstration we present Milk Carton: a system that aids in FTR. Milk Carton creates a registry containing evacuee records. To find separated persons, Milk Carton uses Eigenfaces face recognition to match queries with existing records. Milk Carton uses a clustered architecture of Computing Nodes to handle data storage and execute the Eigenfaces algorithm. To operate under challenged-network environments, Milk Carton uses response patrol vehicles as data ferries to deliver data. In this demonstration, we show how Milk Carton uses Eigenfaces to locate separated persons and how data ferries and Computing Nodes function."
"Memory Efficient One-Sided Communucation Library ""ACP"" in Globary Memory on Raspberry Pi 2.","Previously, communications in parallel programs for High Performance Computing (HPC) and Distributed Computing (DC) are mostly written with two-sided communication interfaces that are based on a pair of operations, Send and Receive. Since such interface requires explicit synchronization between both sides of the communication, techniques for communication optimization such as overlapping are not efficiently described in many cases. On the other hand, one-sided communication interface is becoming important as a method to describe asynchronous communications to enable highly overlapped communication with computation. As one of such interface, in this demonstration, Advanced Communication Primitives (ACP) is introduced. ACP is a portable interface that supports UDP, IBverbs of InfiniBand and Tofu library of K Computer. In addition to that, it is designed to be memory efficient. For example, with 10 thousand processes, the memory consumption of ACP over UDP is estimated to be less than 1MB. Since the number of computational elements is increasing more rapidly than the amount of available memory, this memory efficiency is becoming one of the keys for parallel programs in HPC and DC. To show this characteristics, we run ACP library on Raspberry Pi 2, and examine its performance and memory consumption."
An SDN-Based Multipath GridFTP for High-Speed Data Transfer.,"We demonstrate high-speed data transfer GridFTP using a multipath control mechanism with SDN (Software-Defined Networking). GridFTP is a typical tool that has been developed and widely used for bulk data transfer over a wide area network in the field. GridFTP supports a parallel high-speed data transfer scheme using multiple TCP streams. However, one of the shortest paths is used solely for data transfer in the default IP routing while there are multiple network paths (multipath) exist between widely-distributed sites. In this study, we propose a system that distributes the parallel TCP streams of GridFTP into multiple network paths by a traffic engineering technique brought by SDN. Our proposed system has achieved approximately 20% better performance than the conventional method in the best case in a global-scale real enviroment."
An Endpoint Communication Profiling Tool for Distributed Computing Frameworks.,"Message exchange is a central activity in distributed computing frameworks. Nevertheless, past research has paid little attention on profiling techniques and tools for endpoint communication. In this paper, we fill this gap by introducing a new fine-grained profiler for endpoints and communication between them in distributed systems. Our tool aids efficient analysis, optimization, and tuning of endpoint communication in distributed frameworks. The paper presents an overview of our profiler, discusses employed profiling techniques and collected metrics, and shows preliminary results in profiling a well-known distributed computing framework."
Cruisers: A Public Automotive Sensing Platform for Smart Cities.,"Collecting urban data in a citywide scale plays a fundamental role in the research, development and implementation of smart cities. This demo introduces Cruisers, an automotive sensing platform for smart cities, which is developed based on the following ideas. a) Garbage collecting trucks are used as host automobiles to accommodate sensors, b) 3G cellular communication network is used to wirelessly deliver sensed data directly to servers, and c) Proxy server(s) are adopted to convert the format of sensed data to required ones. This platform has been deployed to 24 garbage collecting trucks at Fujisawa city, i.e., nearly 1/4 of the total number of such trucks in the city. An iOS application is also developed to demonstrate the sensing process and the covered area."
DL-Store: A Distributed Hybrid OLTP and OLAP Data Processing Engine.,"There has been a recent push in the database community towards supporting real-time analytical queries (OLAP) while sustaining a large volume of fine-grained updates (OLTP). Supporting these types of workloads require both an efficient data storage layer as well as a distributed architecture. In this demo, we address the latter point with our Distributed Lineage-based Data Store (DL-Store), which is a distributed data processing engine. DL-Store is built on top of L-Store, which is a lineage-based storage architecture designed to handle mixed OLTP and OLAP workloads, and provides scalability and elasticity by supporting multiple L-Store nodes. To maintain the desired consistency semantics, DL-Store employs a distributed transaction handler component which can horizontally scaled by provisioning additional transaction manager nodes. We leverage partitioning in the record space of the transactions to minimize communication across transaction managers while ensuring consistent execution. The demo shows our implementation of DL-Store over Apache Spark using a variety of use cases."
Middleware for Proximity Distributed Real-Time Processing of IoT Data Flows.,"EdgeComputing and Fog Computing are new paradigms where data processing is executed in or on the edge of networks to mitigate cloud server load. However, EdgeComputing and Fog Computing still need powerful servers on the edge of networks which impose additional costs for deployments. We proposed a platform called IFoT (Information Flow of Things) that efficiently performs distributed processing as well as distribution and analysis of data streams near their sources based on ""Process On Our Own (PO3)"" concept. In IFoT, processing of tasks for cloud servers is delegated to an ad-hoc distributed system consisting of proximity IoT devices for distributed real-time stream processing. In this demonstration, we show a face recognition system for person tracking developed on top of IFoT middleware which locally processes video streams in real-time and in a distributed manner by using computational resources of IoT devices."
Federation of Private IaaS Cloud Providers through the Barter of Resources.,"This paper presents the Fogbow middleware. This middleware addresses cloud federation challenges by providing an additional layer for federation a top each local private IaaS that wants to join the federation. It uses designed-to-federate, internet-friendly technologies like XMPP and it is flexible enough to deal with a wide range of cloud technologies, since it provides a plugin framework for allowing simpler interoperation with a wide range of orchestrators. The plugin framework is also useful to define the behaviour of each member of the federation, providing greater autonomy to these members. A lightweight business model based on barter has been implemented and provides a cheap way to federate small and medium size private IaaS cloud providers."
Publish/Subscribe for Mobile Applications Using Shared Dictionary Compression.,"Publish/Subscribe is known as a scalable and efficient data dissemination mechanism. In a mobile environment, there is an added challenge for the pub/sub system to economizemobile bandwidth, which is especially precious in areas not wellcovered by mobile providers. While well-known compressionmethods such as GZip or Deflate are generally useful in suchsituations, we propose using Shared Dictionary Compression(SDC) to achieve a greater level of bandwidth efficiency. SDCrequires a dictionary, generated upfront, to be shared betweentwo communicating peers before it can be used. We proposea design where brokers forming the pub/sub overlay can be incharge of generating and propagating the shared dictionary. Oursolution employs an adaptive algorithm, executed at the brokers, which creates and maintains the dictionaries over time. Withthis approach, it is possible to reduce the required bandwidth byup to 88% including the introduced dictionary overhead. Ourdemo shows this approach applied to a smartphone applicationcommunicating with a publish/subscribe broker using the MQTTprotocol."
Real-Time Client-Side Phishing Prevention Add-On.,"Since existing solutions for steering users away from phishing websites are typically server-based, they have several drawbacks: they compromise user privacy, are not robust against adaptive attackers who serve different content at different times, and do not provide any guidance to users after flagging a website as a phish. To address these limitations, we present a new phishing prevention system implementing a fast and effective phishing detection technique we developed recently [1]. It is implemented as a client-side application and a browser add-on. It uses information extracted from website visited by the user to detect if it is a phish and warn the user. It also determines the target of the phish and offers to redirect the user there."
